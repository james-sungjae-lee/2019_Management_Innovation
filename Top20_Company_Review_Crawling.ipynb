{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 'your_userid_of_jobplanet'\n",
    "password = 'your_password_of_jobplanet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('./chromedriver')\n",
    "driver.implicitly_wait(3)\n",
    "driver.get('https://www.jobplanet.co.kr/companies?sort_by=review_culture_cache')\n",
    "\n",
    "driver.find_element_by_xpath('/html/body/div[1]/div[1]/header/div/div[2]/a[1]').click()\n",
    "\n",
    "driver.find_element_by_name('user[email]').send_keys(user_id)\n",
    "driver.find_element_by_name('user[password]').send_keys(password)\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"signInSignInCon\"]/div[2]/div/section[2]/fieldset/button').click()\n",
    "\n",
    "base_url = 'https://www.jobplanet.co.kr/companies?sort_by=review_culture_cache&page='\n",
    "company_name_list = []\n",
    "company_score_list = []\n",
    "company_link_list = []\n",
    "\n",
    "page_num = 1\n",
    "\n",
    "for num in range(page_num):\n",
    "    page_url = base_url + str(num)\n",
    "    driver.get(page_url)\n",
    "    driver.implicitly_wait(1)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    company_names = soup.select('#listCompanies > div > div.section_group > section > div > div > dl.content_col2_3.cominfo > dt > a')\n",
    "    company_scores = soup.select('#listCompanies > div > div.section_group > section > div > div > dl.content_col2_4 > dd.gf_row > span')\n",
    "    \n",
    "    for name in company_names:\n",
    "        company_name_list.append(name.text)\n",
    "    for score in company_scores:\n",
    "        company_score_list.append(score.text)\n",
    "    for a_tag in company_names:\n",
    "        company_link_list.append(re.findall('\"(.+?)\"', str(a_tag))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(company_name_list)):\n",
    "    print(company_name_list[i], '\\t', str(company_score_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_url = 'https://www.jobplanet.co.kr'\n",
    "good_text = []\n",
    "bad_text = []\n",
    "\n",
    "for link in company_link_list:\n",
    "    one_url = my_url + link\n",
    "\n",
    "    driver.get(one_url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    comment_nums = int(soup.select('#viewReviewsTitle > span')[0].text)\n",
    "\n",
    "    current_url = driver.current_url\n",
    "\n",
    "    for comment_page_num in range(math.ceil(comment_nums/5)):\n",
    "        comment_url = current_url + '?page=' + str(comment_page_num + 1)\n",
    "\n",
    "        driver.get(comment_url)\n",
    "        driver.implicitly_wait(1)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        good_comments = soup.select('#viewReviewsList > div > div > div > section > div > div.ctbody_col2 > div > dl > dd:nth-of-type(1) > span')\n",
    "        bad_commnets = soup.select('#viewReviewsList > div > div > div > section > div > div.ctbody_col2 > div > dl > dd:nth-of-type(2) > span')\n",
    "        for comment in good_comments:\n",
    "            good_text.append(comment.text)\n",
    "        for comment in bad_commnets:\n",
    "            bad_text.append(comment.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_good_text = ''\n",
    "for text in good_text:\n",
    "    all_good_text += text\n",
    "    all_good_text += '\\n'\n",
    "    \n",
    "all_bad_text = ''\n",
    "for text in bad_text:\n",
    "    all_bad_text += text\n",
    "    all_bad_text += '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"top20_good_text.txt\", \"w\")\n",
    "file.write(all_good_text)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"top20_bad_text.txt\", \"w\")\n",
    "file.write(all_bad_text)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
